{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OAYUyI2wqo0b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mou5RO6Lqo0d"
      },
      "outputs": [],
      "source": [
        "df_en = pd.read_excel(\"Dataset_EFREI_en.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af4woF0Mqo0e"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gI8RoDoLqo0f",
        "outputId": "7715186a-5b7a-47f8-e7a8-671f80ba9b54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('words')\n",
        "\n",
        "words = set(nltk.corpus.words.words())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "kwl4-HzBqo0g"
      },
      "outputs": [],
      "source": [
        "df_en['question_lower'] = df_en['Question'].str.lower()\n",
        "df_en['responce_lower'] = df_en['Answer'].str.lower()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE13MDi3qo0m"
      },
      "source": [
        "# Remove URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "M80Bp5-9qo0m"
      },
      "outputs": [],
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-UGcrBbW9nr"
      },
      "source": [
        "# Remove mentions and hashtags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "UZkOqsaLXPlY"
      },
      "outputs": [],
      "source": [
        "def remove_mentions_hashtags(text):\n",
        "  text = re.sub('@[A-Za-z0-9_]+',\"\", text)\n",
        "  text = re.sub(\"[0-9][A-Za-z0-9_]+\",\"\", text)\n",
        "  text = re.sub('lax',\"\", text)\n",
        "  text = re.sub('flight',\"\", text)\n",
        "  return re.sub(\"#[A-Za-z0-9_]+\",\"\", text)\n",
        "\n",
        "\n",
        "df_en['question_cleaned'] = df_en['question_lower'].apply(lambda text: remove_mentions_hashtags(text))\n",
        "df_en['responce_cleaned'] = df_en['responce_lower'].apply(lambda text: remove_mentions_hashtags(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8dGdMmOqo0h"
      },
      "source": [
        "# Remove Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "nzRTwIxqqo0h"
      },
      "outputs": [],
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "df_en[\"question_punct\"] = df_en[\"question_cleaned\"].apply(lambda text: remove_punctuation(text))\n",
        "df_en[\"responce_punct\"] = df_en[\"responce_cleaned\"].apply(lambda text: remove_punctuation(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxTd1uZLAvVV"
      },
      "source": [
        "# Remove Non English Words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "4O7TMN4sAtbh"
      },
      "outputs": [],
      "source": [
        "def remove_non_english(text):\n",
        "  return \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n",
        "\n",
        "df_en['question_punct'] = df_en['question_punct'].apply(lambda text: remove_non_english(text))\n",
        "df_en['responce_punct'] = df_en['responce_punct'].apply(lambda text: remove_non_english(text))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvo7BauJqo0k"
      },
      "source": [
        "# Remove Emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "zDIamZEpqo0k"
      },
      "outputs": [],
      "source": [
        "def remove_emojis(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U0001F383\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "\n",
        "df_en[\"question_cleaned\"] = df_en[\"question_punct\"].apply(lambda text: remove_emojis(text))\n",
        "df_en[\"responce_cleaned\"] = df_en[\"responce_punct\"].apply(lambda text: remove_emojis(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPi93mvqo0i"
      },
      "source": [
        "# Word Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3UO8bBOqo0i",
        "outputId": "3c5eaf03-5e4f-4dd5-9c2a-eb4ec252c97d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0    what degree represent offer in and embody open...\n",
              "1            degree be in and be open to international\n",
              "2              what be in and be open to international\n",
              "3       what degree be in and be open to international\n",
              "4          what degree in and be open to international\n",
              "Name: question_lemmatized, dtype: object"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word,'v') for word in text.split()])\n",
        "\n",
        "\n",
        "df_en[\"question_lemmatized\"] = df_en[\"question_cleaned\"].apply(lambda text: lemmatize_words(text))\n",
        "df_en[\"responce_lemmatized\"] = df_en[\"responce_cleaned\"].apply(lambda text: lemmatize_words(text))\n",
        "\n",
        "df_en['question_lemmatized'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkVshTx2qo0n"
      },
      "source": [
        "# Word Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "TBXFVkqu21Nq",
        "outputId": "4cc2230f-104a-4083-90cc-c9cd4a9e4208"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenizing(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "df_en['word_question'] = df_en['question_lemmatized'].apply(lambda text: tokenizing(text))\n",
        "df_en['word_responce'] = df_en['responce_lemmatized'].apply(lambda text: tokenizing(text))\n",
        "\n",
        "\n",
        "df_en['word_question'].head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Rd9rk92xd1"
      },
      "source": [
        "# Remove stop-words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gg-yMMNqo0o",
        "outputId": "c5d6c8b3-1ac7-4739-9a68-3fb1f72cef08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgvjBMRZ6DmD",
        "outputId": "619e4458-727f-4dae-a7a9-4fe18010adc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['degree', 'open', 'international']"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def remove_stop_words(words):\n",
        "  return [word for word in words if not word in stopwords.words('english')]\n",
        "\n",
        "df_en['words_question_cleaned'] = df_en['word_question'].apply(lambda text: remove_stop_words(text))\n",
        "df_en['words_responce_cleaned'] = df_en['word_responce'].apply(lambda text: remove_stop_words(text))\n",
        "\n",
        "df_en['words_question_cleaned'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "SwCLnvER12vM"
      },
      "outputs": [],
      "source": [
        "df_en.to_excel('data_en.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGAds4ja0Ghm"
      },
      "source": [
        "#Doc2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "WojmGiBGzYda"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "\n",
        "# Load your dataset\n",
        "df_en = pd.read_excel(\"data_en.xlsx\")\n",
        "\n",
        "# Pre-processing\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    # Remove mentions and hashtags\n",
        "    text = re.sub('@[A-Za-z0-9_]+', '', text)\n",
        "    text = re.sub(\"#[A-Za-z0-9_]+\", '', text)\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Stemming\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to questions and answers\n",
        "df_en['question_tokens'] = df_en['Question'].apply(preprocess_text)\n",
        "df_en['answer_tokens'] = df_en['Answer'].apply(preprocess_text)\n",
        "\n",
        "# Tag documents for Doc2Vec model\n",
        "tagged_data = [TaggedDocument(words=question, tags=[index])\n",
        "               for index, question in enumerate(df_en['question_tokens'])]\n",
        "\n",
        "# Train Doc2Vec model\n",
        "vector_size = 100  # Adjust the vector size based on your dataset and requirements\n",
        "max_epochs = 100   # Increase epochs for better training\n",
        "model = Doc2Vec(vector_size=vector_size, window=2, min_count=1, workers=4, epochs=max_epochs)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"doc2vec_model\")\n",
        "\n",
        "# Load the trained model\n",
        "model = Doc2Vec.load(\"doc2vec_model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF4WxzYEbGs9"
      },
      "source": [
        "#Testing Part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb3_s2VQ1Gai",
        "outputId": "db445e13-c97f-4091-b520-c5c1b5d04326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similar questions for question 0:\n",
            "Similarity: 97.8%\n",
            "Question: degree programs are offered in English and are open to international candidates ?\n",
            "Answer: you will find all information regarding our undergraduate programs at https://eng.efrei.fr/graduate-programs/.\n",
            "followed by a 2 year master degree program in the areas listed at https://eng.efrei.fr/graduate-programs/.\n",
            "\n",
            "Similar questions for question 60:\n",
            "Similarity: 96.3%\n",
            "Question: What are the requirements and list of required admission documents for an exchange research internship ?\n",
            "Answer: please find the details for your application for an exchange program or research internship at https://eng.efrei.fr/international-admission/application-for-an-exchange-program/.\n",
            "\n",
            "Similar questions for question 120:\n",
            "Similarity: 98.5%\n",
            "Question: What are the deadlines to apply an exchange program/ research internship ?\n",
            "Answer: fall semester: may 15\n",
            "spring semester:  october 15 \n",
            "nomination of exchange candidates by their home institution: at least 15 days prior to the deadline.\n",
            "\n",
            "Similar questions for question 180:\n",
            "Similarity: 94.7%\n",
            "Question: much does a master program cost ?\n",
            "Answer: the tuition fees are listed at https://eng.efrei.fr/international-admission/tuition-fees-and-financial-assistance/.\n",
            "\n",
            "Similar questions for question 240:\n",
            "Similarity: 98.2%\n",
            "Question: What kind accommodation is available to students ?\n",
            "Answer: students usually find accommodation in private student residences in paris or the close outskirts. the type of accommodations are either studio apartments with a bathroom and kitchenette or single/double rooms with shared cooking/dining facilities. please find all of the information at https://eng.efrei.fr/practical-information/accommodation-in-paris/.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-86-04e9e745acac>:8: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
            "  similar_questions = model.docvecs.most_similar([new_question_embedding],topn=1)\n"
          ]
        }
      ],
      "source": [
        "# Print similar questions for every +10 question\n",
        "for i in range(0, 250, 60):  # Loop every +10\n",
        "    try:\n",
        "        q = df_en['question_tokens'][i]\n",
        "        new_question_embedding = model.infer_vector(q)\n",
        "\n",
        "        # Use the embedding to retrieve similar questions from the training data\n",
        "        similar_questions = model.docvecs.most_similar([new_question_embedding],topn=1)\n",
        "\n",
        "        print(f\"Similar questions for question {i}:\")\n",
        "        for index, similarity in similar_questions:\n",
        "            print(\"Similarity:\", str(round(similarity*100,1)) + \"%\")\n",
        "            print(\"Question:\", df_en['Question'][index])\n",
        "            print(\"Answer:\", df_en['Answer'][index])\n",
        "            print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing question {i}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJkzOAiaw8cN"
      },
      "source": [
        "#Language Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCeJ7OQf7ZZb"
      },
      "outputs": [],
      "source": [
        "import langdetect\n",
        "from langdetect import detect\n",
        "\n",
        "text1 = \"This is an example of a text in English.\"\n",
        "text2 = \"Ceci est dans une autre langue.\"\n",
        "# Detect the language of the text\n",
        "language1 = detect(text1)\n",
        "language2 = detect(text2)\n",
        "\n",
        "print(\"The language of the text is:\", language1)\n",
        "print(\"The language of the text is:\", language2)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
