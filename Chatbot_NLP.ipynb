{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 89,
      "source": [
        "import pandas as pd"
      ],
      "outputs": [],
      "metadata": {
        "id": "OAYUyI2wqo0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "source": [
        "df_en = pd.read_excel(\"Dataset_EFREI_en.xlsx\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "mou5RO6Lqo0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "af4woF0Mqo0e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('words')\n",
        "\n",
        "words = set(nltk.corpus.words.words())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "metadata": {
        "id": "gI8RoDoLqo0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b9c082-518d-472e-9ed8-51c92bfc88e8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "source": [
        "df_en['question_lower'] = df_en['Question'].str.lower()\n",
        "df_en['responce_lower'] = df_en['Answer'].str.lower()\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "kwl4-HzBqo0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove URLs"
      ],
      "metadata": {
        "id": "jE13MDi3qo0m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "M80Bp5-9qo0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove mentions and hashtags"
      ],
      "metadata": {
        "id": "A-UGcrBbW9nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_mentions_hashtags(text):\n",
        "  text = re.sub('@[A-Za-z0-9_]+',\"\", text)\n",
        "  text = re.sub(\"[0-9][A-Za-z0-9_]+\",\"\", text)\n",
        "  text = re.sub('lax',\"\", text)\n",
        "  text = re.sub('flight',\"\", text)\n",
        "  return re.sub(\"#[A-Za-z0-9_]+\",\"\", text)\n",
        "\n",
        "\n",
        "df_en['question_cleaned'] = df_en['question_lower'].apply(lambda text: remove_mentions_hashtags(text))\n",
        "df_en['responce_cleaned'] = df_en['responce_lower'].apply(lambda text: remove_mentions_hashtags(text))\n"
      ],
      "metadata": {
        "id": "UZkOqsaLXPlY"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Punctuation"
      ],
      "metadata": {
        "id": "e8dGdMmOqo0h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "df_en[\"question_punct\"] = df_en[\"question_cleaned\"].apply(lambda text: remove_punctuation(text))\n",
        "df_en[\"responce_punct\"] = df_en[\"responce_cleaned\"].apply(lambda text: remove_punctuation(text))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "nzRTwIxqqo0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Non English Words\n"
      ],
      "metadata": {
        "id": "YxTd1uZLAvVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_english(text):\n",
        "  return \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n",
        "\n",
        "df_en['question_punct'] = df_en['question_punct'].apply(lambda text: remove_non_english(text))\n",
        "df_en['responce_punct'] = df_en['responce_punct'].apply(lambda text: remove_non_english(text))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4O7TMN4sAtbh"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Emojis"
      ],
      "metadata": {
        "id": "Pvo7BauJqo0k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "source": [
        "def remove_emojis(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U0001F383\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "\n",
        "df_en[\"question_cleaned\"] = df_en[\"question_punct\"].apply(lambda text: remove_emojis(text))\n",
        "df_en[\"responce_cleaned\"] = df_en[\"responce_punct\"].apply(lambda text: remove_emojis(text))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "zDIamZEpqo0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Lemmatizer"
      ],
      "metadata": {
        "id": "EXPi93mvqo0i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word,'v') for word in text.split()])\n",
        "\n",
        "\n",
        "df_en[\"question_lemmatized\"] = df_en[\"question_cleaned\"].apply(lambda text: lemmatize_words(text))\n",
        "df_en[\"responce_lemmatized\"] = df_en[\"responce_cleaned\"].apply(lambda text: lemmatize_words(text))\n",
        "\n",
        "df_en['question_lemmatized'][1]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'degree be in and be open to international'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "y3UO8bBOqo0i",
        "outputId": "4961a4a4-9aa4-4bcc-c1e6-19622731eac8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Tokenize"
      ],
      "metadata": {
        "id": "SkVshTx2qo0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenizing(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "df_en['word_question'] = df_en['question_lemmatized'].apply(lambda text: tokenizing(text))\n",
        "df_en['word_responce'] = df_en['responce_lemmatized'].apply(lambda text: tokenizing(text))\n",
        "\n",
        "\n",
        "df_en['word_question'].head"
      ],
      "metadata": {
        "id": "TBXFVkqu21Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove stop-words\n"
      ],
      "metadata": {
        "id": "T4Rd9rk92xd1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "metadata": {
        "id": "7gg-yMMNqo0o",
        "outputId": "c5d6c8b3-1ac7-4739-9a68-3fb1f72cef08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(words):\n",
        "  return [word for word in words if not word in stopwords.words('english')]\n",
        "\n",
        "df_en['words_question_cleaned'] = df_en['word_question'].apply(lambda text: remove_stop_words(text))\n",
        "df_en['words_responce_cleaned'] = df_en['word_responce'].apply(lambda text: remove_stop_words(text))\n",
        "\n",
        "df_en['words_question_cleaned'][1]"
      ],
      "metadata": {
        "id": "kgvjBMRZ6DmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619e4458-727f-4dae-a7a9-4fe18010adc0"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['degree', 'open', 'international']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_en.to_excel('data_en.xlsx')"
      ],
      "metadata": {
        "id": "SwCLnvER12vM"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Doc2Vec model"
      ],
      "metadata": {
        "id": "kGAds4ja0Ghm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "\n",
        "# Load your dataset\n",
        "df_en = pd.read_excel(\"data_en.xlsx\")\n",
        "\n",
        "# Pre-processing\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    # Remove mentions and hashtags\n",
        "    text = re.sub('@[A-Za-z0-9_]+', '', text)\n",
        "    text = re.sub(\"#[A-Za-z0-9_]+\", '', text)\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Stemming\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to questions and answers\n",
        "df_en['question_tokens'] = df_en['Question'].apply(preprocess_text)\n",
        "df_en['answer_tokens'] = df_en['Answer'].apply(preprocess_text)\n",
        "\n",
        "# Tag documents for Doc2Vec model\n",
        "tagged_data = [TaggedDocument(words=question, tags=[index])\n",
        "               for index, question in enumerate(df_en['question_tokens'])]\n",
        "\n",
        "# Train Doc2Vec model\n",
        "vector_size = 100  # Adjust the vector size based on your dataset and requirements\n",
        "max_epochs = 100   # Increase epochs for better training\n",
        "model = Doc2Vec(vector_size=vector_size, window=2, min_count=1, workers=4, epochs=max_epochs)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"doc2vec_model\")\n",
        "\n",
        "# Load the trained model\n",
        "model = Doc2Vec.load(\"doc2vec_model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WojmGiBGzYda"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing with Questions from Dataset"
      ],
      "metadata": {
        "id": "lF4WxzYEbGs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print similar questions for every +10 question\n",
        "c = 0\n",
        "for i in range(0, 250, 60):  # Loop every +10\n",
        "    c += 1\n",
        "    try:\n",
        "        q = df_en['question_tokens'][i]\n",
        "        new_question_embedding = model.infer_vector(q)\n",
        "\n",
        "        # Use the embedding to retrieve similar questions from the training data\n",
        "        similar_questions = model.docvecs.most_similar([new_question_embedding],topn=1)\n",
        "\n",
        "        print(f\"Similar questions for question {c}:\")\n",
        "        for index, similarity in similar_questions:\n",
        "            print(\"Similarity:\", str(round(similarity*100,1)) + \"%\")\n",
        "            print(\"Question:\", df_en['Question'][index])\n",
        "            print(\"Answer:\", df_en['Answer'][index])\n",
        "            print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing question {i}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb3_s2VQ1Gai",
        "outputId": "43c532fc-6697-43cd-9f02-29ba16b12ef1"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar questions for question 1:\n",
            "Similarity: 97.8%\n",
            "Question: degree programs are offered in English and are open to international candidates ?\n",
            "Answer: you will find all information regarding our undergraduate programs at https://eng.efrei.fr/graduate-programs/.\n",
            "followed by a 2 year master degree program in the areas listed at https://eng.efrei.fr/graduate-programs/.\n",
            "\n",
            "Similar questions for question 2:\n",
            "Similarity: 96.3%\n",
            "Question: What are the requirements and list of required admission documents for an exchange research internship ?\n",
            "Answer: please find the details for your application for an exchange program or research internship at https://eng.efrei.fr/international-admission/application-for-an-exchange-program/.\n",
            "\n",
            "Similar questions for question 3:\n",
            "Similarity: 98.5%\n",
            "Question: What are the deadlines to apply an exchange program/ research internship ?\n",
            "Answer: fall semester: may 15\n",
            "spring semester:  october 15 \n",
            "nomination of exchange candidates by their home institution: at least 15 days prior to the deadline.\n",
            "\n",
            "Similar questions for question 4:\n",
            "Similarity: 94.7%\n",
            "Question: much does a master program cost ?\n",
            "Answer: the tuition fees are listed at https://eng.efrei.fr/international-admission/tuition-fees-and-financial-assistance/.\n",
            "\n",
            "Similar questions for question 5:\n",
            "Similarity: 98.2%\n",
            "Question: What kind accommodation is available to students ?\n",
            "Answer: students usually find accommodation in private student residences in paris or the close outskirts. the type of accommodations are either studio apartments with a bathroom and kitchenette or single/double rooms with shared cooking/dining facilities. please find all of the information at https://eng.efrei.fr/practical-information/accommodation-in-paris/.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-101-d76979e58281>:10: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
            "  similar_questions = model.docvecs.most_similar([new_question_embedding],topn=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing with Questions outside Dataset"
      ],
      "metadata": {
        "id": "OU9C-cIv7Yfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"what are deadline ?\"\n",
        "q = preprocess_text(question1)\n",
        "new_question_embedding = model.infer_vector(q)\n",
        "\n",
        "similar_questions = model.dv.most_similar([new_question_embedding],topn=1)\n",
        "\n",
        "print(f\"Similar questions for question : \" + question1)\n",
        "for index, similarity in similar_questions:\n",
        "    print(\"Similarity:\", str(round(similarity*100,1)) + \"%\")\n",
        "    print(\"Question:\", df_en['Question'][index])\n",
        "    print(\"Answer:\", df_en['Answer'][index])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEPsRe-f7X-f",
        "outputId": "3119fcca-8214-48cb-8afd-449bc2c0141c"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar questions for question : what are deadline ?\n",
            "Similarity: 98.8%\n",
            "Question: What be the deadline to apply for AN exchange program/ research internship ?\n",
            "Answer: fall semester: may 15\n",
            "spring semester:  october 15 \n",
            "nomination of exchange candidates by their home institution: at least 15 days prior to the deadline.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Detection"
      ],
      "metadata": {
        "id": "xJkzOAiaw8cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langdetect\n",
        "from langdetect import detect\n",
        "\n",
        "text1 = \"This is an example of a text in English.\"\n",
        "text2 = \"Ceci est dans une autre langue.\"\n",
        "# Detect the language of the text\n",
        "language1 = detect(text1)\n",
        "language2 = detect(text2)\n",
        "\n",
        "print(\"The language of the text is:\", language1)\n",
        "print(\"The language of the text is:\", language2)\n",
        "\n"
      ],
      "metadata": {
        "id": "tCeJ7OQf7ZZb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.10 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}