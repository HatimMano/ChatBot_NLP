{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import pandas as pd"
      ],
      "outputs": [],
      "metadata": {
        "id": "OAYUyI2wqo0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "df_en = pd.read_excel(\"Dataset_EFREI_en.xlsx\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "mou5RO6Lqo0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "af4woF0Mqo0e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('words')\n",
        "\n",
        "words = set(nltk.corpus.words.words())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ],
      "metadata": {
        "id": "gI8RoDoLqo0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7715186a-5b7a-47f8-e7a8-671f80ba9b54"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "source": [
        "df_en['question_lower'] = df_en['Question'].str.lower()\n",
        "df_en['responce_lower'] = df_en['Answer'].str.lower()\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "kwl4-HzBqo0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove URLs"
      ],
      "metadata": {
        "id": "jE13MDi3qo0m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "M80Bp5-9qo0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove mentions and hashtags"
      ],
      "metadata": {
        "id": "A-UGcrBbW9nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_mentions_hashtags(text):\n",
        "  text = re.sub('@[A-Za-z0-9_]+',\"\", text)\n",
        "  text = re.sub(\"[0-9][A-Za-z0-9_]+\",\"\", text)\n",
        "  text = re.sub('lax',\"\", text)\n",
        "  text = re.sub('flight',\"\", text)\n",
        "  return re.sub(\"#[A-Za-z0-9_]+\",\"\", text)\n",
        "\n",
        "\n",
        "df_en['question_cleaned'] = df_en['question_lower'].apply(lambda text: remove_mentions_hashtags(text))\n",
        "df_en['responce_cleaned'] = df_en['responce_lower'].apply(lambda text: remove_mentions_hashtags(text))\n"
      ],
      "metadata": {
        "id": "UZkOqsaLXPlY"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Punctuation"
      ],
      "metadata": {
        "id": "e8dGdMmOqo0h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "df_en[\"question_punct\"] = df_en[\"question_cleaned\"].apply(lambda text: remove_punctuation(text))\n",
        "df_en[\"responce_punct\"] = df_en[\"responce_cleaned\"].apply(lambda text: remove_punctuation(text))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "nzRTwIxqqo0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Non English Words\n"
      ],
      "metadata": {
        "id": "YxTd1uZLAvVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_english(text):\n",
        "  return \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n",
        "\n",
        "df_en['question_punct'] = df_en['question_punct'].apply(lambda text: remove_non_english(text))\n",
        "df_en['responce_punct'] = df_en['responce_punct'].apply(lambda text: remove_non_english(text))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4O7TMN4sAtbh"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Emojis"
      ],
      "metadata": {
        "id": "Pvo7BauJqo0k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "source": [
        "def remove_emojis(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U0001F383\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "\n",
        "df_en[\"question_cleaned\"] = df_en[\"question_punct\"].apply(lambda text: remove_emojis(text))\n",
        "df_en[\"responce_cleaned\"] = df_en[\"responce_punct\"].apply(lambda text: remove_emojis(text))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "zDIamZEpqo0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Lemmatizer"
      ],
      "metadata": {
        "id": "EXPi93mvqo0i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word,'v') for word in text.split()])\n",
        "\n",
        "\n",
        "df_en[\"question_lemmatized\"] = df_en[\"question_cleaned\"].apply(lambda text: lemmatize_words(text))\n",
        "df_en[\"responce_lemmatized\"] = df_en[\"responce_cleaned\"].apply(lambda text: lemmatize_words(text))\n",
        "\n",
        "df_en['question_lemmatized'].head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    what degree represent offer in and embody open...\n",
              "1            degree be in and be open to international\n",
              "2              what be in and be open to international\n",
              "3       what degree be in and be open to international\n",
              "4          what degree in and be open to international\n",
              "Name: question_lemmatized, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3UO8bBOqo0i",
        "outputId": "3c5eaf03-5e4f-4dd5-9c2a-eb4ec252c97d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Tokenize"
      ],
      "metadata": {
        "id": "SkVshTx2qo0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenizing(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "df_en['word_question'] = df_en['question_lemmatized'].apply(lambda text: tokenizing(text))\n",
        "df_en['word_responce'] = df_en['responce_lemmatized'].apply(lambda text: tokenizing(text))\n",
        "\n",
        "\n",
        "df_en['word_question'].head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "TBXFVkqu21Nq",
        "outputId": "4cc2230f-104a-4083-90cc-c9cd4a9e4208"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of 0      [what, degree, represent, offer, in, and, embo...\n",
              "1      [degree, be, in, and, be, open, to, internatio...\n",
              "2       [what, be, in, and, be, open, to, international]\n",
              "3      [what, degree, be, in, and, be, open, to, inte...\n",
              "4      [what, degree, in, and, be, open, to, internat...\n",
              "                             ...                        \n",
              "630        [what, if, the, embassy, be, in, my, country]\n",
              "631     [what, if, the, embassy, be, close, my, country]\n",
              "632     [what, if, the, embassy, be, close, in, country]\n",
              "633          [what, if, the, embassy, be, close, in, my]\n",
              "634    [what, if, the, embassy, be, close, in, my, co...\n",
              "Name: word_question, Length: 635, dtype: object>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.generic.NDFrame.head</b><br/>def head(n: int=5) -&gt; NDFrameT</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py</a>Return the first `n` rows.\n",
              "\n",
              "This function returns the first `n` rows for the object based\n",
              "on position. It is useful for quickly testing if your object\n",
              "has the right type of data in it.\n",
              "\n",
              "For negative values of `n`, this function returns all rows except\n",
              "the last `|n|` rows, equivalent to ``df[:n]``.\n",
              "\n",
              "If n is larger than the number of rows, this function returns all rows.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "n : int, default 5\n",
              "    Number of rows to select.\n",
              "\n",
              "Returns\n",
              "-------\n",
              "same type as caller\n",
              "    The first `n` rows of the caller object.\n",
              "\n",
              "See Also\n",
              "--------\n",
              "DataFrame.tail: Returns the last `n` rows.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; df = pd.DataFrame({&#x27;animal&#x27;: [&#x27;alligator&#x27;, &#x27;bee&#x27;, &#x27;falcon&#x27;, &#x27;lion&#x27;,\n",
              "...                    &#x27;monkey&#x27;, &#x27;parrot&#x27;, &#x27;shark&#x27;, &#x27;whale&#x27;, &#x27;zebra&#x27;]})\n",
              "&gt;&gt;&gt; df\n",
              "      animal\n",
              "0  alligator\n",
              "1        bee\n",
              "2     falcon\n",
              "3       lion\n",
              "4     monkey\n",
              "5     parrot\n",
              "6      shark\n",
              "7      whale\n",
              "8      zebra\n",
              "\n",
              "Viewing the first 5 lines\n",
              "\n",
              "&gt;&gt;&gt; df.head()\n",
              "      animal\n",
              "0  alligator\n",
              "1        bee\n",
              "2     falcon\n",
              "3       lion\n",
              "4     monkey\n",
              "\n",
              "Viewing the first `n` lines (three in this case)\n",
              "\n",
              "&gt;&gt;&gt; df.head(3)\n",
              "      animal\n",
              "0  alligator\n",
              "1        bee\n",
              "2     falcon\n",
              "\n",
              "For negative values of `n`\n",
              "\n",
              "&gt;&gt;&gt; df.head(-3)\n",
              "      animal\n",
              "0  alligator\n",
              "1        bee\n",
              "2     falcon\n",
              "3       lion\n",
              "4     monkey\n",
              "5     parrot</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 5474);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove stop-words\n"
      ],
      "metadata": {
        "id": "T4Rd9rk92xd1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "metadata": {
        "id": "7gg-yMMNqo0o",
        "outputId": "c5d6c8b3-1ac7-4739-9a68-3fb1f72cef08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(words):\n",
        "  return [word for word in words if not word in stopwords.words('english')]\n",
        "\n",
        "df_en['words_question_cleaned'] = df_en['word_question'].apply(lambda text: remove_stop_words(text))\n",
        "df_en['words_responce_cleaned'] = df_en['word_responce'].apply(lambda text: remove_stop_words(text))\n",
        "\n",
        "df_en['words_question_cleaned'][1]"
      ],
      "metadata": {
        "id": "kgvjBMRZ6DmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619e4458-727f-4dae-a7a9-4fe18010adc0"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['degree', 'open', 'international']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_en.to_excel('data_en.xlsx')"
      ],
      "metadata": {
        "id": "SwCLnvER12vM"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Doc2Vec model"
      ],
      "metadata": {
        "id": "kGAds4ja0Ghm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "\n",
        "# Load your dataset\n",
        "df_en = pd.read_excel(\"data_en.xlsx\")\n",
        "\n",
        "# Pre-processing\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    # Remove mentions and hashtags\n",
        "    text = re.sub('@[A-Za-z0-9_]+', '', text)\n",
        "    text = re.sub(\"#[A-Za-z0-9_]+\", '', text)\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Stemming\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to questions and answers\n",
        "df_en['question_tokens'] = df_en['Question'].apply(preprocess_text)\n",
        "df_en['answer_tokens'] = df_en['Answer'].apply(preprocess_text)\n",
        "\n",
        "# Tag documents for Doc2Vec model\n",
        "tagged_data = [TaggedDocument(words=question, tags=[index])\n",
        "               for index, question in enumerate(df_en['question_tokens'])]\n",
        "\n",
        "# Train Doc2Vec model\n",
        "vector_size = 100  # Adjust the vector size based on your dataset and requirements\n",
        "max_epochs = 100   # Increase epochs for better training\n",
        "model = Doc2Vec(vector_size=vector_size, window=2, min_count=1, workers=4, epochs=max_epochs)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"doc2vec_model\")\n",
        "\n",
        "# Load the trained model\n",
        "model = Doc2Vec.load(\"doc2vec_model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WojmGiBGzYda"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Part"
      ],
      "metadata": {
        "id": "lF4WxzYEbGs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print similar questions for every +10 question\n",
        "for i in range(0, 250, 60):  # Loop every +10\n",
        "    try:\n",
        "        q = df_en['question_tokens'][i]\n",
        "        new_question_embedding = model.infer_vector(q)\n",
        "\n",
        "        # Use the embedding to retrieve similar questions from the training data\n",
        "        similar_questions = model.docvecs.most_similar([new_question_embedding],topn=1)\n",
        "\n",
        "        print(f\"Similar questions for question {i}:\")\n",
        "        for index, similarity in similar_questions:\n",
        "            print(\"Similarity:\", str(round(similarity*100,1)) + \"%\")\n",
        "            print(\"Question:\", df_en['Question'][index])\n",
        "            print(\"Answer:\", df_en['Answer'][index])\n",
        "            print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing question {i}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb3_s2VQ1Gai",
        "outputId": "db445e13-c97f-4091-b520-c5c1b5d04326"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar questions for question 0:\n",
            "Similarity: 97.8%\n",
            "Question: degree programs are offered in English and are open to international candidates ?\n",
            "Answer: you will find all information regarding our undergraduate programs at https://eng.efrei.fr/graduate-programs/.\n",
            "followed by a 2 year master degree program in the areas listed at https://eng.efrei.fr/graduate-programs/.\n",
            "\n",
            "Similar questions for question 60:\n",
            "Similarity: 96.3%\n",
            "Question: What are the requirements and list of required admission documents for an exchange research internship ?\n",
            "Answer: please find the details for your application for an exchange program or research internship at https://eng.efrei.fr/international-admission/application-for-an-exchange-program/.\n",
            "\n",
            "Similar questions for question 120:\n",
            "Similarity: 98.5%\n",
            "Question: What are the deadlines to apply an exchange program/ research internship ?\n",
            "Answer: fall semester: may 15\n",
            "spring semester:  october 15 \n",
            "nomination of exchange candidates by their home institution: at least 15 days prior to the deadline.\n",
            "\n",
            "Similar questions for question 180:\n",
            "Similarity: 94.7%\n",
            "Question: much does a master program cost ?\n",
            "Answer: the tuition fees are listed at https://eng.efrei.fr/international-admission/tuition-fees-and-financial-assistance/.\n",
            "\n",
            "Similar questions for question 240:\n",
            "Similarity: 98.2%\n",
            "Question: What kind accommodation is available to students ?\n",
            "Answer: students usually find accommodation in private student residences in paris or the close outskirts. the type of accommodations are either studio apartments with a bathroom and kitchenette or single/double rooms with shared cooking/dining facilities. please find all of the information at https://eng.efrei.fr/practical-information/accommodation-in-paris/.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-86-04e9e745acac>:8: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
            "  similar_questions = model.docvecs.most_similar([new_question_embedding],topn=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Detection"
      ],
      "metadata": {
        "id": "xJkzOAiaw8cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langdetect\n",
        "from langdetect import detect\n",
        "\n",
        "text1 = \"This is an example of a text in English.\"\n",
        "text2 = \"Ceci est dans une autre langue.\"\n",
        "# Detect the language of the text\n",
        "language1 = detect(text1)\n",
        "language2 = detect(text2)\n",
        "\n",
        "print(\"The language of the text is:\", language1)\n",
        "print(\"The language of the text is:\", language2)\n",
        "\n"
      ],
      "metadata": {
        "id": "tCeJ7OQf7ZZb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.10 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}